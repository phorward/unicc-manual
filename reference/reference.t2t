
+ Using UniCC +

++ Overview ++

The UniCC parser generator is a software program that compiles augmented parser definitions into program-modules of a higher programming language or optionally into a structured, not further targetted parser description file.

A parser definition analyzed by UniCC is an ASCII (UTF-8) formatted textfile that generally contains definitions for //terminal symbols//, //nonterminal symbols// and //productions// to describe a context-free grammar. These definitions are expressed in a Backus-Naur-Form-styled notation.

In most cases, a parser definition also contains several configuration directives, or simply called //directives//. They are used for the configuration of symbol-, task-, generation- and augmentation-related features. There are also a few directives that must be defined before any directive or grammar construction is done, because they influence general settings that cannot be changed later on. These directives are called //top-level directives//.
 
Additionally, a parser definition file can contain operational programming code that is revised by UniCC and inserted into appropriate positions within the resulting program module. These code fragments have the purpose to fit a particular need within the parsing process. Code blocks can be specified to various parser directives, to productions and some special terminal definitions.

Due the target-language independency of the UniCC parser generator and its parser definition language, a parser definition file can also contain additional information called //tags//. These tags can be defined globally or associated with various grammatical objects. Use of this feature is in the interest of subsequent, from UniCC detached tasks which perform operations on the output of UniCC and the use this additional information for various purposes or results.

++ Installation ++

There are four installation packages for download available on the Phorward Software Technologies website, currently only targetting the Linux and Windows platform in both a 32-Bit and a 64-Bit x86 CPU.

- UniCC for Linux, Kernel 2.6.27 and higher, x86 (unicc-1.0rc1_x86-linux.tar.gz)
- UniCC for Linux, Kernel 2.6.27 and higher, x86-64 (unicc-1.0rc1_x86_64-linux.tar.gz)
- UniCC for Windows, 32-Bit, x86 (unicc-1.0rc1_x86-win.zip)
- UniCC for Windows, 64-Bit, x86-64 (unicc-1.0rc1_x86_64-win.zip)


After extracting the archive of the given platform's binary package, a directory containing the UniCC binary executable, the UniCC Standard C Parser Template (C.tlt) and several files is made available. The directory can be extracted to any desired location, but this demands on the use.

The root directory contains the LICENSE and README files, as well as an environment setup script for Linux (setenv) and Windows (setenv.bat). This script can be called from a shell or command-prompt to setup the current environment for the use with UniCC.

On Linux, the invocation is done with

``` . ./setenv

Under Windows, invocation is simply done with

``` setenv.bat

After calling the environment setup script, the variable **PATH** is extended to the UniCC binary directory, and the variable **UNICC_TPLDIR** directs to the //tlt// directory, containing the target language templates. If there will be more target language templates available in future, they can be integrated by simply copying the .tlt-files into this directory. Also, development of own or modified target language templates can be done here.

//doc// contains the UniCC User's Manual (this file!) as PDF document and //unicc.man//, the README as Unix-style manpage file.
//samples// contains some example grammars, and will also grow in future.

To make UniCC permanently available on a specific system, the apropriate variables need to be set in the system's configuration.
On Linux, this can be done in the system file ///etc/profile//, under Windows in the system settings dialog for environment variables.

++ Building UniCC from source ++[ref_compile_from_source]

To build UniCC from its source code, some further tasks are required. The UniCC LALR(1) Parser Generator is a product that was entirely established and developed on top of the [Phorward Foundation Libraries http://phorward.phorward-software.com]. The Phorward Foundation Libraries are a collection of libraries, utility programs and shell scripts to provide a platform-independent development environment and build system for C programs. The main purpose of this project is to serve as a build-environment and consistent basic toolkit for various software projects having their origin at Phorward Software Technologies. Especially the platform-independent approach and many of the useful tools, libraries and functions provided by the Phorward Foundation Libraries make the entire package suitable for many C programmers, beyond Phorward project development only.

The Phorward Foundation Libraries are released under the BSD License, and can be compiled under POSIX-like systems (with gcc under Linux, BSD, Solaris) as well as Windows (using Microsoft Visual C++ Express Edition and with the assistance of a Unix-like shell emulation environment, like [Msys http://www.mingw.org/wiki/MSYS]). Instructions on system requirements and how to setup the Phorward Foundation Libraries can be found in the related README file and on the [Phorward Software website http://phorward.phorward-software.com].

The Phorward Foundation Libraries, the UniCC LALR(1) Parser Generator and the UniCC Standard C Parser Template are developed using the [Mercurial SCM http://mercurial.selenic.com/] as version control system, so this method will also be used in the examples here. Using this method will always guarantee that the latest sources (including inofficial and experimental features) are used, but Mercurial also allows to check-out any commit of the UniCC sources after fetching them once. There are source code packages of UniCC as snapshots for all release versions also. They are extracted using a related archiving program (gunzip, tar), but the procedure of building UniCC from source remains the same as described here.

First of all, checking out (or extracting) the Phorward Foundation Libraries is required. This can be done after installing Mercurial by typing

``` hg clone http://phorward.hg.sourceforge.net:8000/hgroot/phorward/phorward

in a console. Setting up the Phorward Foundation Libraries for Unix-like systems or Windows is then done by typing

```
cd phorward
. run/psetup
```

and then invoking

``` . ./psh

to configure the environment for their use with the Phorward Foundation Libraries.
By typing

``` s

then, the shell will change to the source directory, where the UniCC sources and utilities can now be checked out with the commands

```
hg clone http://unicc.hg.sourceforge.net:8000/hgroot/unicc/unicc
hg clone http://unicc.hg.sourceforge.net:8000/hgroot/unicc/Cparser
hg clone http://unicc.hg.sourceforge.net:8000/hgroot/unicc/xpl
```

Now, the creation of some files is required to setup the environment for UniCC's own bootstrap.
The following commands will create a permanent user-defined environment script that is applied the next time the Phorward Foundation Libraries environment is set up, and also updates the current environment to get the variable UNICC_TPLDIR known.

```
test ! -x "$PHOME/etc/userenv" && echo '#!/bin/sh' >$PHOME/etc/userenv
echo 'export UNICC_TPLDIR=$PHOME/src/Cparser' >>$PHOME/etc/userenv
chmod +x $PHOME/etc/userenv
. $PHOME/etc/userenv
```
After this, a new project folder build order needs to be specified using

``` echo "phorward min_lalr1 Cparser unicc xpl" >makeall-seq

This will create an extended build-sequence for the makeall script, which is
part of the Phorward Foundation Libraries.

Finally, typing

``` makeall

will compile all dependencies and UniCC.
After successfull compilation, typing

``` unicc

will invoke the UniCC LALR(1) Parser Generator. The executable is put into //$PHOME/run//, and can be copied anywhere else if desired. The UniCC Standard C Parser Template will be build in //$PHOME/src/Cparser/C.tlt//, and can be moved or linked elsewhere.

The README file and UniCC manpage can be generated with

``` make doc

in the UniCC directory, with the use of the [txt2tags http://www.txt2tags.org] software (it needs to be installed and in the PATH).
Building a function reference documentation of the Phorward Foundation Libraries can be generated to //$PHOME/doc// by running

``` makeall doc

@NEWPAGE@

++ Invocation and command-line options ++[ref_commandline_parameters]

UniCC primarily provides a command-line interface to invoke the parser generation process.
The general calling convention of the UniCC parser generator is

``` unicc OPTIONS... filename.par

This command-line interface supports various, combinable options to invoke, modify and specialize the parser generation process, or to trigger further tasks.

|| Option | Long option name | Description |
| -a | --all-warnings | Runs UniCC to print all warnings that come up with the grammar. UniCC normaly supresses some warning messages that raise up during the parse table constructions according to their importance. |
| -b //name// | --basename //name// | Defines the specified basename //name// to be used for the output file(s) instead of the one derived by the ``#prefix`` directive or by the name of the input filename. This basename is used for all output files if the provided parser template causes the construction of multiple files. |
| -G | --grammar | Dumps an overview of the finally constructred grammar to stderr, right before the parse-tables are generated. |
| -h | --help | Prints a short overview about the command-line options and exits. |
| -V | --version | Prints copyright and version information and exists. |
| -n | --no-opt | Disables state optimization; By default, the resulting LALR(1) parse states are optimized during table construction by introducing a special SHIFT_REDUCE action which combines a shift and reduction, which is possible when the last symbol of a production is shifted. Standard LALR(1) parsers only support SHIFT or REDUCE, not both operations at the same time. When this option is used, UniCC produces about 20-30% more LALR(1) states. |
| -P | --production | Dumps an overview about the finally produced productions and their semantic actions. |
| -s | --stats | Prints a statistics message to stderr when parser generation has entirely been finished. |
| -S | --states | Dumps the generated LALR(1) states that had been generated during the parse table generation process. |
| -t | --stdout | Prints all generated output to stdout instead of files. |
| -T | --symbols | Dumps an overview of all used symbols. |
| -v | --verbose | Prints process messages about the specific tasks during parser generation process. Also turns on ``-s``. |
| -w | --warnings | Print relevant warnings. |
| -x | --xml | Triggers UniCC to run the [parser description file generator #ref_codegen_descfile] additionally to the program module generator. The parser description file generator outputs an XML-based parser representation of the generated parse tables, which can be used by third-party code generators or grammar analsys and debugging tools. |
| -X | --XML | Triggers UniCC to only run the [parser description file generator #ref_codegen_descfile] without running the program-module generator. |


TABLE#ref_command_line_options#The UniCC command-line interface options.

Errors and warnings are printed to STDERR, any other kind of output to STDOUT.

%-------------------------------------------------------------------------------
%-------------------------------------------------------------------------------
%-------------------------------------------------------------------------------
@NEWPAGE@

++ Code generators ++[ref_codegen]

CENTER
''' IMAGE#generators.png#An overview about the UniCC code-generators.
/CENTER

UniCC can not only be seen as a parser generator to compile a parser definition into a piece of program code to implement this parser. It is also a parser generator that can be used as the base for different ("any") kinds of parser analyzation, code generation and optimization issues. This is the reason why UniCC comes with two integrated code-generators: One code generator that builds program-modules expressed in a particular programming language, and one code generator to build an independent parser description file that describes the compiled form of the grammar and a transparent representation of the output parser.
The output of the first code-generator can directly be fed to a compiler, whereas the output of latter code-generator can be analyzed by any type of other program with a specialized purpose.

+++ The program-module generator +++[ref_codegen_progmod]
The //program-module generator// is the default code generator that is used by UniCC if nothing else is explicitly specified. It is used to build a parser-module in a specific high-level programming language. Thanks to its template-based approach, this code-generator is not targetted to one specific programming language. All target-language-related code is read from tags defined in a parser template file, which must follow a static structure that is pretended by the program-module generator in order to construct the output code.

The standard C parser template provided with UniCC is a parser template of such kind. It gives UniCC the ability to build parsers written in the C programming language which can be compiled after generation without any further modification. If more parser templates will exist somewhere in the future, UniCC will also be capable to generate program-modules for parsers written in other programming languages, like C++, C#, Java, Pascal, Fortran or anything else.

Given the very simple grammar
%!include: ``reference/simple.par``

UniCC constructs a C program that consists of more than 1000 lines of code using the program-module generator in combination with the standard C parser template. This output source can directly be passed to a standard C/C++ compiler like //gcc// without any further modification.

%This is the output program module:
%%!include: ``reference/simple.c``
%And this is the related header-file, in case of the [UniCC standard C parser template#stdcparser]:
%%!include: ``reference/simple.h``

+++ The XML-based parser description generator +++[ref_codegen_descfile]
The //parser description generator// outputs an XML-based representation of the generated parser. This parser description file contains the LALR(1) parse tables, tables for the lexical analyzer, conditioned semantic code, a structured listing of all symbols and productions with all of its tagged information, the original parser definition source and any warning or error messages produced by UniCC during the parser construction process.

Third-party programs can work on this information to generate individual parser code or code-parts, directly interpret, analyze, modify, represent or rewrite the compiled parser for any desired purpose.

To trigger the parser description generator, UniCC must be run with the //-x// or //-X// command-line option. //-x// runs both the program-module generator and the parser description generator, //-X// will only run the parser description generator.

With the very simple grammar from above,
%!include: ``reference/simple.par``

UniCC causes to build a grammar definition file like this when using the parser description file generator:
%!include: ``reference/simple.xml``

All semantic code parts and their macros and variables are split into several tags which can be easily adapted, modified or enhanced.
For example, the following production definition

```
variable<int*>  -> 'a-z':name   [* @@ = &variables[ @name - 'a' ]; *]
```

is compiled into the following XML structure

```
<production id="20" length="1" defined-at="55">
	<left-hand-side symbol-id="21" offset="0" />
	<right-hand-side symbol-id="37" offset="0" named="name" />
	<code defined-at="55">
		<raw> </raw>
		<variable target="left-hand-side" value-type="int*" value-type-id="0" />
		<raw> = &amp;variables[ </raw>
		<variable target="right-hand-side" offset="0" />
		<raw> - 'a' ]; </raw>
	</code>
</production>
```

which can be easily converted into another representation.

The entire document type definition (DTD) of the UniCC Parser Description Files is printed in the [appendix 2 #appendix2] of this manual.

%-------------------------------------------------------------------------------
%-------------------------------------------------------------------------------
%-------------------------------------------------------------------------------
@NEWPAGE@

++ Parser construction modes ++[ref_construction_modes]
UniCC is a flexible parser generator that can handle two different methods to construct its parsers and their lexical analyzators.

The first and default method is called the //sensitive// parser construction mode. This construction mode is a speciality of UniCC, and gives a maximum of flexibility to implement parsers for nearly any type of context-free language. UniCC analyzes and rewrites the grammar according to several rules influencing whitespace and lexeme detection and separation. The lexical analysis, including whitespace, can be broken down to single input characters to enable full context-free grammars on lexem level this way. Lexical analysis is still done silently, but with the option that there is no direct cut between lexer and parser required.

The second method, called //insensitive// parser construction mode, always uses one single lexical analyzer that identifies terminal symbols. The difference to the sensitive mode is, that lesser states are produced, because the grammar is not rewritten, and whitespace is directly absorbed within the stage of lexical analysis. Overlapping character-classes can not be used in this mode. This construction mode can be compared to most other parser generators like the one used by the combination of //lex// and //yacc//.

It depends on the requirements of the grammar which construction mode should be used. The specialities on the two construction methods are described below. The construction mode can be changed with the ``#!mode`` top-level directive.

+++ Sensitive mode +++[ref_sensitive_mode]
The whitespace sensitive parser construction mode gives a maximum of flexibility on whitespace and lexeme construction and their behavior, and is used by default. It is a UniCC-speciality that was never provided by any other parser generator before in this way.

The most common characteristic of this construction mode is, that UniCC entirely rewrites the grammar according to whitespace and lexeme definitions, to make whitespace only valid in selected situations. The definition of whitespace is not limited to one terminal symbol anymore in this mode. The symbol defining whitespace can be a nonterminal described as part of the context-free grammar itself without any limitations. An optional call to this nonterminal is added behind every terminal symbol and in front of the goal symbol during the grammar revision. To disallow whitespace in particular constructs, some nonterminals can be defined as lexemes. They are covered as terminal symbols during grammar revision in order to the whitespace-matching, but can also consist of any context-free grammar.

The advantage of this construction mode is, that whitespace and lexemes can be expressed in a context-free grammar rather than as regular strings matched by the lexical analyzer. The lexical analysis apparently becomes part of the parser with all its possibilites, but the grammar is expressed as whitespace would be handled apart from it. Additionally, the true lexical analyzer can be optionally used to parse the atomic terminals, as part of lexemes or on its own.

To make this parsing approach possible, overlapping character terminals are made unique and split up into nonterminals. A lexical analyzer is constructed individually for every LALR state, to only match symbols that are valid in the sensitive context of the given state. Some grammars that use this mode may cause a high number of states and many different lexical analyzers. But it enables the ultimative maximum of flexibility ever provided by a LALR(1) parsed grammar.

As an example, then following grammar is given.

%!include: ``reference/sensitive.par``

``whitespace`` is configured as whitespace nonterminal here, ``integer`` is a lexem that is triggered as a coherent lexical unit, where not whitespace is allowed within. Using this grammar to parse the input string //18 * 2//, the following syntax tree will be constructed. The nonterminals ``&whitespace*``, ``&whitespace+`` and ``&whitespace`` are automatically inserted by UniCC. Also the symbol ``\t-\n #``, likewise a generated nonternimal symbol, has been inserted by UniCC during character-class separation. Grammars modified by UniCC are heavier to read, but they gain this unique features in flexibility.

GENAST#reference/sensitive.par#18 * 2##Syntax tree of the string "18 * 2" using a sensitively constructed parser.

The underlying, rewritten grammar that is generated by UniCC can be dumped by using the //-G// command-line option.

+++ Insensitive mode +++[ref_insensitive_mode]
The whitespace insensitive parser construction mode can be compared to most existing parser generators. It strictly separates the part of the lexical analysis from the parser. Whitespace is only handled and consumed by the lexical analyzer. Every terminal symbol is concerned as a lexical unit standing on its own. The grammar is not rewritten nor modified by UniCC, and results in a faster parser with lesser states.

The disadvantage of this parser construction mode is, that the whitespace-sensitive aspect, enabling the full control of any whitespace situation and speciality gets lost. Anyway, parser constructed in insensitive mode can be used for most parsing issues.

Trying to compile above grammar using ``#!mode insensitive;`` will throw some errors because this grammar uses features that can only be handled in insensitive mode.

%!include: ``reference/insensitive.par``

In insensitive mode, it is only allowed to use one terminal symbol as whitespace. This terminal symbol can be a character-class, a string or a regular expression definition. Nonterminal symbols can't be configured to be used as whitespace. The use of the lexeme-directive becomes effectless, so the former lexem nonterminal ``integer`` must be rewritten to a regular-expression terminal ``@integer`` here.

Parsing the same expression //18 * 2// yields in the following, much smaller syntax tree. The whitespace handling is not covered by the parser anymore, and is run "silently" in the background within the lexical analyzer.

GENAST#reference/insensitive.par#18 * 2##Syntax tree of the string "18 * 2" using an insensitively constructed parser.

%-------------------------------------------------------------------------------
%-------------------------------------------------------------------------------
%-------------------------------------------------------------------------------

+ UniCC Grammar Definitions +

++ Comments ++
Commenting grammars is possible everywhere in the grammar code. UniCC supports the C++-styled standard comment forms, which are ``/*...*/`` for block comments and ``//`` for single line comments, to ignore all the rest of a line.
```
//Define goal symbol
example$ -> "PRINT" expr
         |  "IF" expr "THEN" example ';' //Now we can do IFs! :)
         // | "START" expr
         |  "READ" var
         /*
            We want to implement this later:
         |  "GOTO" line
         |  "GOSUB" line
         */
         ;
```
It must be annotated, that this commenting style is only possible within the grammar code itself. Comments in semantic code blocks must be expressed in the commenting style of the respective target programming language that is used. These comments will also be copied into the resulting parser program-module.

++ Escape sequences ++[ref_escape_sequences]
UniCC supports ANSI-C-styled escape-sequences within any string or value that is used in the grammar or any directive.
The following table provides a listing of all available escape sequences.

|| Escape sequence | Description |
| ``\a`` | Bell (alert) |
| ``\b`` | Backspace |
| ``\f`` | Formfeed |
| ``\n`` | New line |
| ``\r`` | Carriage return |
| ``\t`` | Horizontal tab |
| ``\v`` | Vertical tab |
| ``\'`` | Single quotation mark |
| ``\"`` | Double quotation mark |
| ``\\`` | Backslash |
| ``\OOO`` | ASCII character in octal notation, (O = octal digit) |
| ``\xHH`` | ASCII character in hexadecimal notation (H = hexadecimal digit) |
| ``\uHHHH`` | 32-Bit Unicode character in hexadecimal notation (H = hexadecimal digit) |
| ``\UHHHHHHHH`` | 64-Bit Unicode character in hexadecimal notation (H = hexadecimal digit) |


TABLE#ref_escape_sequences#Global escape sequences to be used in UniCC.

++ Definition blocks ++[ref_def_block]
The UniCC grammar definition language provides blocks of several definitions. Each block has an introductional symbol, e.g. an identifier for a nonterminal definition, or a particular parser configuration directive, and ends with a semicolon (``;``).

```
//Confguration directive block
#lexeme int ;

//Nonterminal definition block
int -> '0-9'+ ;

//Terminal definition block (for terminals based on a regular expression)
@id name 'A-Za-z_'+ ;
```

Any other type of definition is invalid, and will result in a parse error.

@NEWPAGE@

++ Grammars ++
Grammars are expressed using rules that describe in which way symbols may appear. The symbols within a grammar define grammatical units in form of //terminal symbols// (terminals) which immediatelly are expected in the input, and //nonterminal symbols// (nonterminals). Nonterminal symbols union one or more grammatical rules, so called //productions// into one "fictive" symbol to be inserted into other productions, or to form the goal symbol. Every symbol (both terminal and nonterminal) in UniCC is always declared by its first use. Its particular description or definition - if required - can be made elsewhere later in the grammar. Symbols without a definition will be reported by UniCC, and maybe stop the further compilation process of the grammar, with a request for correction.

+++ Terminal symbols +++
Terminal symbols (terminals) define a piece of raw input data that is read from the input stream. They can be defined and specified in various ways. A [single character or a set of valid characters #ref_terminal_ccl] can form a terminal, but also a [static character sequence #ref_terminal_string] or character sequences that matches to a regular (type-0) language, defined via [regular expressions #ref_terminal_regex].

All terminal symbols are identified by an lexical analyzer. Every terminal symbol yields in a leaf within the resulting parse tree, it has no children.

++++ Characters ++++[ref_terminal_ccl]
Character terminals, also called //character-classes//, are the simplest definition type of a terminal symbol. This kind of terminal symbol defines a character or a set of valid input characters.

A character-class is specified by strings enclosed in single quotation marks  ``'...'``, and is defined by its use. Once a character-class terminal is used somewhere in the grammar or any directive, it is defined and becomes known to UniCC.

To match only one individual character, for example an **a**, then ``'a'`` is the correct definition for such a terminal. To define all characters from ``a`` to ``f`` as valid input, ``'abcdef'`` and ``'a-f'`` as range definition is possible. To define a terminal matching the characters from ``a`` to ``z``, ``0`` to ``9`` and the ``=`` equal sign, ``'a-z0-9='`` can be specified. If the range is negative, for example ``'z-a``, its still interpretered as ``'a-z'``. To define the dash **-** itself as part of the character class, it the best to express it as first character of the character-class string, or in a place where no dash is interpreted as range definition, for example ``'-+*/'`` or ``'a-z-+'``.

UniCC accepts [escape sequences #ref_escape_sequences] to define special characters and unprinable characters. UTF-8 characters are directly accepted. For example ``'\n-â‚¬'`` is a valid definition with an escape-sequence for newline and the Euro currency sign.

Character terminals can be negated using an preceding exclamation-mark. For example ``!'a-z'`` accepts all characters except the range from ``a`` to ``z`` (this will include all characters in the [UniCC character universe #ref_terminal_charuniverse], from 0x0 to 0xFFFF, except above range).

In the UniCC Standard C Parser Template, a character-terminal is always associated with the data-type ``int`` containing the character-code of the matched character. This value can be used in semantic actions.

``` digit<int> -> '0-9':dig     [* @@ = @dig - '0'; *]

It depends on the used implementation language and parser template which data-type is used for terminals expressed by character-classes.

++++ String sequences ++++[ref_terminal_string]
String terminals define a sequence of characters, which must exactly match to the string specified in the current input.

They are defined similarly to character-terminals, but are enclosed by double quotation marks ``"..."``. The same escape sequences as in character-terminals can be used. ``"while"`` is an example for a string definition matching a keyword. ``"#special operation"`` defines a string with a blank; If this blank is not given in the input, the string sequence is not matched. If two blanks are given, the string sequence is even not matched. Such a string definition overrides any whitespace definition, the blank is mandatory in this situation, and handled as part of the lexical analysis, rather than the grammar.

Internally, string terminals are handled like terminals based on regular expressions, and have the strongest level of specialization within the lexical analysis. This means, that a string-terminal that exactly fits to a current input string is matched __before__ any regular expression terminal matching the same string does.

String terminals don't have a semantic data-type association. They stand on their own and represent a static lexical unit, in most cases some kind of keyword.

With the use of the [``#case-insensitive strings`` #ref_case_insensitive_strings] directive, string terminals can be configured to ignore upper-/lower case order, if case-conversion is possible with the characters they are made up from.

++++ Regular expressions ++++[ref_terminal_regex]
Terminal symbols based on regular expressions are the most flexible style of expressing a terminal symbol. They can be made-up of a fully-fledged type-0 regular language. Regular expressions must explicitly be defined in their own definition block within the grammar, using the following syntax.

``` @identifier regular-expression-term [*semantic actions for the lexical analysis*];

The ``@`` character introduces the regular expression, both in its definition and wherever the symbol is used.
The regular-expression itself is defined by sequences of the following, already known syntactical elements.

|| Construct | Usage |
| ``'...'`` or ``!'...'`` | Specifies a character, character-class or negated character-class. |
| ``"..."`` | Specifies a static string. |
| ``.`` | Specifies a character-class standing for "any character". Using this construct causes the terminal to be configured as "non-greedy".
| ``(`` and ``)`` | Parantheses to build sub-expressions, equal to [embedded productions #ref_embedded_productions]. |
| ``|`` | The alternative operator to define multiple expressions at one expression level. |
| ``*`` | Kleene closure (none or several of previous expression) modifier. |
| ``+`` | Positive closure (one or several of previous expression) modifier. |
| ``?`` | Optional closure (none or one of previous expression) modifier. |


TABLE#ref_regex_construction#Lexical symbols for regular expression construction

To get more familar with this syntax, a few examples follow.

```
//A simple identifier!
@identifier 'A-Za-z_'+ ;

//Extended identifier...
@extidentifier 'A-Za-z_' 'A-Za-z0-9_'* ;

//A fictive example with sub-expressions would be
@hello "Hello" ' \t'+ ( "World" | !'A-Za-z' )? ;

//A string value
@string '"' .* '"' ;
```

The order of the definition of terminals based on regular expression also indicates their level of specialization, so specialized regular expressions shall be defined first in the grammar. It is strongly recommended to describe all regular expressions before the first nonterminal definition is done, to avoid unexpected behaviors, which are having they origin in wrong definition orders in most cases.

Note, that terminals based on regular expression can always replace a character terminal or string terminal. Regular expression terminals are sorted behind string terminals, but before character terminals in their specialization order.

```
@abc     'abc'
         ;

example$ -> @abc
         | 'abc' //this production will never be matched!
         ;
```

It is obvious, that terminals which are defined via regular expressions are a very powerful tool.
But they are also often the source for unexpected parser behaviors, that let some grammar developers become desperate in many situations. Especially in the sensitive parser construction mode, regular expressions should __really__ be handled with care.

+++++ Semantic actions +++++
Terminals defined via regular expressions can also be equipped with individual semantic code and a data-type for semantic augmentation, to return an individual semantic value to the reduction actions used in the grammar.
This semantic code can be used for different purposes, also simultaneausly: To extract semantic values, to modify the input, or to perform semantic symbol selections.

Same as within semantic reducton codes of productions, some macros within this semantic code block for termnals can be used to access the return value and the matched input string. Their meaning is not the same in all target languages, it depends on the target language template.

|| Macro | Usage |
| ``@@`` | Defines the return value that is associated with the terminal. This variable is of the same type that is specified for the symbol or the default type.
| ``@>`` | Defines the start of the string. In the C standard template parser, this is a char-pointer to the first byte of the matched string.
| ``@<`` | Defines the end of the string. In the C standard template parser, this is a char-pointer to the last byte of the matched string.
| ``@!symbol:name`` | Sets the returned symbol to //name//, for semantic symbol selections. |


TABLE#ref_regex_semantic_macros#Semantic macros in regular expressions.

Some examples in C:
```
//Match an integer
@integer<int>    '0-9'+

    [* @@ = atoi( @> ); *]
    ;

//Match a string
@string<char*>   '"' !'"'* '"'

    [*
       @@ = @>;
    *]
    ;
```

In various target languages, the semantic action code of regular expression terminals may be the source for memory leaks if they are used faulty. Depending on the used parser target language template and the parser mode, the semantic part of a regular expression terminal can be executed multiple times, to allow for semantic symbol selections, but also to extract semantic values from the input, as shown above. For such cases, every parser template sets some variables or pre-processor directives to turn-off areas within the semantic code which may cause memory problems. The documentation of the particular UniCC parser target language template should handle this topic.

+++++ #greedy, #non-greedy: Defining greedyness +++++
Terminal symbols based on regular expressions can be defined to work //greedy// or //non-greedy//, relating to their behavior during the process of lexical analysis.
UniCC automatically defines a terminal that uses the character-class ``.`` for any character as non-greedy. The greedyness can also be overwritten and explicitly defined using the terminal symbol configuration directives ``#greedy`` and ``#non-greedy``.

To describe the problem, the following grammar is used.

```
#!mode insensitive;
#!language "C";

#whitespaces @WHITESPACE;

@WHITESPACE     ' \t\r\n'+
                | "/*" .* "*/"
                ;

@int            '0-9'+      
                [* printf( "int %s\n", @> ); *]
                ;


start$ -> @int ;
```

There is a whitespace definition for comments reading

``` "/*" .* "*/"

This definition will cause a regular expression terminal reading a comment that begins with ``/*`` and ends with ``*/``. The ``.*`` defines none or several characters of any kind. In this special case, UniCC switches the nonterminal ``@WHITESPACE`` to be non-greedy, due to the use of the ``.`` placeholder.

Now, using the input

``` 3 /* Hello */ 4 /* World */ 5

One would expect, that ``3``, ``4`` and ``5`` are returned by the above grammar, and the comments are ignored. A non-greedy configuration of ``@WHITESPACE``, as it is automatically set by UniCC in this example would do this. But a "greedy" configuration of ``@WHITESPACE`` would only return ``3`` and ``5``, because a comment that is ``/* Hello */ 4 /* World */`` will be identified. This first ``*/`` is also recognized in greedy-mode terminals, but the lexical analyzer will scan further until it (maybe) gets a second ``*/``.

To enable full control about if such regular expression-based terminals are configured to be greedy or non-greedy, UniCC offers the directives ``#greedy`` and ``#nongreedy``, that had been mentioned above.

Changing above definition of ``@WHITESPACE`` to be read as

```
@WHITESPACE     ' \t\r\n'+
                | "/*" .* "*/"
                #greedy
                ;
```

Would scan up the way in the second, greedy case. ``#greedy`` and ``#non-greedy`` are attached right before the definition's end marker (``;``) but behind possible semantic code.
Switchting terminals to be greedy or non-greedy may cause entirely different parse trees and results, so that the use of this tool should only be done carefully.

+++++ Semantic terminal determination +++++[ref_semantic_terminal_determination]
UniCC supports a possibility to perform semantic terminal determination in lexical semantic actions. This possibility is well implemented and tested, but has some caveats to know about when used in the sensitive parser construction mode in combination with programming languages that don't have an automatic memory management (like C). It is more secure to use the method of [semantic nonterminal determination #ref_semantic_nonterminal_determination] as described below, which has no disadvantages according to the lexical way. So this topic is only shortly discussed here, and it is advised to use the nonterminal way of semantic terminal determination.

In some cases, there are terminal symbols that are build-up from the same input sequence, but require some more semantic checks to definitely decide which symbol matches. There are also cases, where the lexical value of a token has a special meaning to the grammar, e.g. a function name referenced in a compiler's symbol table. For such cases, UniCC features semantic symbol determinations within the lexical analyzer. All symbols that match to a given regular expression are unioned with one expression. Then, the semantic action code can perform checking tasks and set the matched symbol to one of the associated symbols. The association is done with the macro ``@!symbol:<name>``, where name defines the symbol to be returned.

Given the example, that an empty string and a nonempty string shall be matched as single symbols, this can be handled with a semantic symbol selection.

```
//Match a string or empty string
@string empty_string <char*>   '"' !'"'* '"'

    [*
        /* Select symbol! */		
        if( strlen( @> ) > 2 )
            @!symbol:string;
        else
            @!symbol:empty_string;

/* 
    Run below code only when shifting, and not only at token detection.
    (this is one of the caveats in the standard C parser template when using
    semantic terminal determination with dynamic memory allocation)
*/
#if UNICC_ON_SHIFT
        @@ = strdup( @> );
#endif
    *]
    ;
```

++++ Terminal anomalies ++++[ref_terminal_anomalies]
When using the [whitespace sensitive parser construction mode #ref_sensitive_mode], terminals could be broken down to grammatical constructs parsing lexemes. This sometimes raises a problem that is known as a terminal anomaly. This terminal anomly can occur between regular expression-based terminals (these are terminal symbols, that are based on strings or regular expressions) and grammar constructs build on character-classes that may consume the same input sequence like the regular expression-based terminal does. In such a situation, the parser may run into an ambiguitiy conflict that matches both the content of a lexeme that is build of the same characters as the valid regular expression-based terminal does.

The following example defines a grammar that generates this kind of conflict.

%!include: ``reference/anomaly.par``

If this is compiled with ``unicc -w``, the following warnings are raising up.

%!include: ``reference/anomaly.out``

UniCC detects these conflicts by testing suspicious nonterminals against all regular expression-based terminals. These messages declare, that the input for //ELSE// and //ENDIF// is both recognized by the nonterminal ``ident`` and their relating string terminals, defining them as keywords. Due to UniCCs build-in terminal precedence leveling, the string terminals will always take a higher precedence than the character-class terminals, but maybe the grammar developer gets an unwanted parse result. This warning helps to detect and fix this issue.

To avoid this warning, the ``#reserve terminals`` directive shall be used. It disables terminal anomaly detection and always assumes that regular expression terminals take precedence over any lexemes.

Terminal anomalies do not raise up in insensitive mode parsers.

%-------------------------------------------------------------------------------
%-------------------------------------------------------------------------------
%-------------------------------------------------------------------------------
@NEWPAGE@

+++ Nonterminal symbols +++
Nonterminal symbols (nonterminals) can be seen as "grammar variables". They always cause a branch to a subsequent grammatical structure within the parse tree, and represent nodes to other nonterminals, which are nodes as usual, or terminals, which are leafs of the tree.

Nonterminals contain one or more //productions//. Productions form sequences of terminal and nonterminal symbols, describing the syntax to which the defined nonterminal can be expanded to. Everytime a nonterminal appears within a production, all its specific rules are valid and possible at the particular situation in the sequence.

Both the nonterminal symbol and its productions are described as a grammar definition block, and follow a variant of the Backus-Naur-Form meta language to describe context-free languages.

``` nonterminal -> production1 | production2 | productionN... ;

The nonterminal's name that should be defined appears on the "left-hand side". Left-hand side means "left from the arrow sign ``->``".
A grammar definition block unions the definition of nonterminals, their productions and the symbol sequences within the productions, which in turn can be definitions of terminal symbols. Everything which is production related is made on the "right-hand side", so right of the arrow sign ``->``.

Here are some examples to follow.

```
//We want to use one regex-terminal
@name 'A-Za-z'+ ;

//Defining a goal symbol with one production
example$ -> hello empty world ;

//'hello', 'empty' and 'world' are nonterminals declared above.

//Now their definition follows:
hello -> "Hello" ;
world -> "World" | @name ; //Two productions.
empty -> ; //A nonterminal with an empty production.
```

A nonterminal definition can also be split into several grammar definition blocks; everytime the nonterminal appears on the left-hand side, all the defined productions are associated to it.

++++ The goal-symbol ++++
A special case is the //goal-symbol//. This nonterminal symbol defines the root of the parse tree, and must exist in every grammar. When the goal-symbol is finally reduced, the parse tree is completed and input was successfully tested for correctness. The goal-symbol is defined by appending a dollar-sign ``$`` to the nonterminal to be the goal-symbol on the left-hand side, like below with

``` example$ -> hello empty world ;

Only one goal symbol is allowed per grammar. Multiple goal symbol definitions throw an error, and no parser will be constructed.

++++ Semantic nonterminal determination ++++[ref_semantic_nonterminal_determination]
There may be cases, where the distinct determination of a nonterminal requires more semantic checks to define a symbol's correct meaning. This can be the case if a grammar allows for function and variable names that are made up of character sequences. There may be a construct within the grammar to recognize the identifier, e.g. a regular expression terminal ``@ident``. But there is no  possibility on the grammar definition level to clearly define wether a parsed identifier refers to a function name or to a variable name. For this case, UniCC supports semantic nonterminal determinations. The semantic code block triggers some kind of selection task, for example a symbol table call, to find out if the addressed identifier is the name of a variable or a function. Then, the semantic code selects the kind of nonterminal to be used in the parser. The feature of semantic nonterminal determination allows to perform a dynamic manipulation of the parser within semantic actions.

This mechanism is done by declaring a production with multiple left-hand sides. The first specified left-hand side will always be used as default. The semantic selection is specified in the reduction code using a special macro ``@!symbol:<name>``, where ``<name>`` refers to the name of the left-hand side symbol to be set.

For the problem described above, the following grammar would be the adequate solution.

```
@ident<char*>   'A-Za-z_'+   [* @@ = @>; *]
                ;

//Two productions are used here
value$          -> variable | function
			    ;

//Multiple nonterminal definition with semantic symbol determination
variable
function	    -> @ident	[*
                                /*
                                    Return symbol 'function' if this is the
                                    name of a function in the symbol table...
                                */
                                if( is_function( @ident ) )
                                    @!symbol:function;
                            *]
                ;
```

A similar mechanism can also be implemented within the use of [lexical analyzer terminal determination #ref_semantic_terminal_determination], but is not advised. Refer the topic to get more information about related caveats.

%-------------------------------------------------------------------------------
%-------------------------------------------------------------------------------
%-------------------------------------------------------------------------------
@NEWPAGE@

+++ Productions +++
A production defines a sequence of symbols that is valid in the context the production may appear in. Every production is associated with a nonterminal symbol, a nonterminal symbol in turn is made up of one or multiple productions (see above).

Productions can only be defined within a grammar definition block as part of a nonterminal definition, by using a Backus-Naur-Form-compliant syntax. All productions a nonterminal is constructed from are defined on the right of the arrow-sign ``->``, which is the reason why a production is also called //the right-hand side// of a grammatical rule. In turn, the nonterminal it belongs to is called //the left-hand side//. Multiple production definitions on the right-hand side are separated by pipes (``|``), or must be stated into a new grammar definition block.

``` nonterm -> hello | world ;

equals to

```
nonterm -> hello ;
nonterm -> world ;
```

When the parser of a defined grammar is executed, it reads terminal-symbols from the input according the sequence rules defined by the grammar's productions. Every symbol (both terminal and nonterminal symbol) is shifted, which means it is consumed and put on a stack when its matched in the input.
When one production's sequence is exactly met, and all its symbols had been shifted onto the stack, it will be reduced. To reduce a production means, that the parser validated the input according to a production's rules as valid. It then replaces the production's sequence by its left-hand side nonterminal, which is part of the next, subsequent production, or the goal-symbol, which defines the input to be valid and stops the parser. All this shifting and reducing is done on an internal stack within the parser, which holds the current parser state. The parser is oriented on the parse-tables constructed by UniCC. They predict which terminal symbols are valid in the current context and which action should be performed next.

A production may also exist of no symbol sequence. These productions are called //epsilon productions//. Their appearance performs an immediate reduction of the nonterminal, if no other production mets. This is an example for an nonterminal with two productions. Latter one is an epsilon production.

``` nonterm -> hello | ;

++++ Semantic actions ++++
The reduction of a production means, that a new node in the parse-tree is constructed. The terminal symbols within the production's rule will become leafs, where the nonterminal symbols become nodes to subsequent, previously reduced rules. Every symbol within the parse tree can be augmented with user-defined data.

For example, a terminal symbol ``@integer`` may hold the integer value of the analyzed integer number. When a production is defined as ``@integer '+' @integer``, and matches the current sequence handle, a reduction is caused. Right before this is done, there are three symbols on the stack: An integer number, the operator **+**, and another integer number. Because every symbol within the parse tree can be augmented with a value, the semantic value behind this sequence can be calculated right when the parse tree is constructed. This means: The programmer is able to put individual code to every production, which can pass values to the newly constructed node and to upperlying productions.

This reduction code is written as a code-block ``[* ... *]`` behind the sequence that defines the production. Within each code-block, UniCC provides a set of macros to access the left-hand side (the "return value" of the rule) and the right-hand side items, respective every symbol of the sequence on the reduced right-hand side.

|| Macro | Usage |
| ``@@`` | Defines the semantic value to be associated with the left-hand side. It can be seen as the "return value" of the production. |
| ``@<offset>`` | Access the semantic values of right-hand side symbols via they offset. |
| ``@<name>``, ``@"<name>"``, ``@'<name>'`` | Access right-hand side via their speaking alias names, which can be an identifier or a string with blanks and other, special characters, when put in quotation marks. |
| ``@!symbol:<name>`` | Specifiy semantic-value dependent nonterminal within a production's reduction code. |


TABLE#ref_production_macros#Semantic macros to be used within reduction actions.

The reduction value (also refered as the //left-hand side value//) can be accessed with the macro ``@@``.

```
boolean$ -> "true"  [* @@ = 1; *]
         |  "false" [* @@ = 0; *]
         ;
```

There are several ways to access symbols on the right-hand side. The simplest is to access them by their offset of appearance. This is done with the variables ``@1``, ``@2``, ``@3`` etc. UniCC validates all used semantic macros within reduction code blocks, so if there is an access to offset 3 in a production that has only two symbols, it will drop an error.

A simple, augmented grammar:
```
//Get integer from input
@integer    '0-9'+                 [* @@ = atoi( @> ); *];

//Parse tiny expressions
example$ -> expr                   [* printf( "= %d\n", @1 ); *]
         ;

expr     -> @integer '+' @integer  [* @@ = @1 + @3; *]
         |	@integer '-' @integer  [* @@ = @1 - @4; *] //<- Error!
         ;
```

UniCC does also support the feature of providing individual reference identifiers for every symbol of the right-hand side, using the syntax ``symbol:identifier``. For example ``'a-z':char`` or ``@name:ident`` would be adequate right-hand side identifiers. Its also possible to provide long-strings, for example ``'0-9'+:"A special number"``.

In case a nonterminal or regular-expression-based terminal appears, an identifying name for it is automatically associated with the same name as the symbol, so no identifier must be provided manually. Note, that this automatism fits only to the first occurence of the symbol on the particular right-hand side. If the same symbol appears multiple times on the same right-hand side, its first occurence can be accessed with this default identifier only.

Identified symbols can then by accessed by ``@identifier``, ``@"Identifier String"`` or ``@'Identifier String'`` within the reduction code. The offset-access is always possible and can be mixed, as below.

```
example$ -> expr                         [* printf( "= %d\n", @expr ); *]
         | @integer:"Hello Folks!"       [* printf( "int: %d\n",
                                                    @"Hello Folks!" ); *]
         ;

expr     -> @integer:i1 '+' @integer:i2  [* @@ = @i1 + @i3; *]
         |	@integer '-' @integer:i2     [* @@ = @integer - @3; *]
         |	'-' @integer                 [* @@ = -@1; *]
         ;
```

Some target programming languages, like C, are strongly typed. If there's no special data type given, the default type specified by the UniCC standard C parser template is ``int``. UniCC provides a way to assign data-types to nonterminals, and this breaks down to production level and the reduction code. If we assume a nonterminal ``ident`` is made up of several characters, its return type should be a ``char*`` that points to a constructed string buffer. Nonterminal ``ident`` can them simply be declared to hold data-type ``char*``.

```
string<char*>    -> 'A-Za-z_'           [* @@ = string_create();
                                           @@ = string_add_char( @@, @1 );
                                        *]
                 |  string 'A-Za-z_'    [* @@ = string_add_char( @@, @1 ); *]
                 ;
```

``string`` is always of type ``char*`` when its used, so constructions like

``` statement -> "print" string         [* printf( "%s\n", @string ); *];

are possible. In other target language templates, this must not be a problem; it strongly relies on the target language template.

In some cases, a default action to set nonterminal values by default is wanted, if no action is assigned to the particular production. By using the ``#default action`` and ``#default epsilon action`` directives, such a default code block can be specified for both production with a sequence and for epsilon productions.

```
#default action         [* @@ = @1; *];
#default epsilon action [* @@ = ' '; *];

example$ -> abc 	    [* printf( ">%c<\n", @1 ); *];
abc      -> 'a' | 'b' | 'c' | ;
```

++++ Virtual productions ++++
UniCC provides a virtual production feature, which is very useful to prototype a grammar quickly. Virtual productions are created when the modifiers ``*``, ``+`` and ``?`` are used. These modifiers can be assigned to any symbol, which is virtually turned into a nonterminal with some productions then.

|| Modifier | Meaning |
| ``*`` | Kleene closure (none or several of previous expression) modifier |
| ``+`` | Positive closure (one or several of previous expression) modifier |
| ``?`` | Optional closure (none or one of previous expression) modifier |


TABLE#ref_virtual_modifiers#Virtual production modifiers.

Each of these modifiers expand into one or in case of the Kleene-closure two automatically derived nonterminal symbols that provide the desired language construct when UniCC compiles the grammar.

A simple example is to parse integer symbols, like
``` integer -> '0-9'+ ;

To allow for a nonterminal ``statement`` in several times, one could write
```
statements_or_null -> statements | ;
statements -> statement statements | statement ;
```
or one could say
``` statements_or_null -> statement* ;

Virtual productions have one disadvantage: Their expanding content can't be augmented with semantic code. This is because their most common use is in language prototyping, or in places where no special semantic operations on the input are required. All default semantic action blocks for productions and empty productions are automatically assigned and executed.

++++ Anonymous nonterminals ++++[ref_anonymous_nonterminals]
There are several situations where the grammar developer requires to create a new nonterminal that is only used once to build-up a desired grammatical construct. Other situations require the creation of a nonterminal with only one empty production just to perform semantic tasks within another production __before__ this production is reduced.
For this special kind of situation, UniCC provides the feature of //anonymous nonterminals//, which can be defined right in place where they are required. An anonymous nonterminal is stated on a right-hand side by surrounding its productions with brackets ``(`` and ``)``. The nonterminal has no naming or alias, an automatically generated name will be assigned by UniCC. Within the brackets, there is the ordinary way of defining productions, separated by the already-known ``|``-separator (pipe). A data-type preceding the brackets allows to define a data-type for the anonymous nonterminal.

```
var_type     -> <BOOLEAN>(
                    "default"
                         [* @@ = TRUE; *]
                    | //This is empty!
                         [* @@ = FALSE; *] ):default

                @ident ';'
                
                [*
                     printf( "var: %s %s\n",
                            @default ? "default" : "", @ident );
                *]
                ;
```

Within the semantic action code of productions defined within anonymous nonterminals, access to the semantic values of all the right-hand side items in front of the appearance of the anonymous nonterminal is possible. The values are intermixed with the semantic values defined within the anonymous production.

A real-life example is this one, to stack-up a value temporarily within a right-hand side. It defines an anonymous nonterminal with only one empty production and a reduction code.

```
procedure_def   -> "def" @ident:funcname

                    //Need to stack the current_depth value!
                    <int>( 
                    
                        /*
                            This is an anonymous nonterminal with an empty
                            production, to be used as "embedded semantic code".
                        */
                        [*  @@ = pcb->current_depth++;
                            fprintf( pcb->debugfile, "New procedure >%s< %d\n",
                                      @funcname, pcb->current_depth ); *]

                         ):depth

                    '{' statements* '}'
                  
                    [*
                        /*
                            This is the usual production's semantic block!
                            Do some code generation here, then replace the
                            stacked value!
                        */
                        pcb->current_depth = @depth;
                    *]
                ;
```

In some parser generators, this feature is called "embedded actions", but in UniCC, the generic term "anonymous nonterminals" is used for both embedded actions and embedded productions.

%-------------------------------------------------------------------------------
%-------------------------------------------------------------------------------
%-------------------------------------------------------------------------------
@NEWPAGE@

++ Directives ++
//Directives// are used to configure and influence various behaviors and settings of UniCC according to parsing, rewriting or constructing the grammar and its parse tables. Directives do always begin with a ``#``-symbol (hash), similar to C-preprocessor directives. A special kind of directive are the "top-level directives", which begin with the sequence ``#!``.

Directives may appear anywhere in the grammar definition, except top-level directives, because they modify options and parameters of the fundamental behavior of UniCC or the grammar, which affect any subsequential directive or definition. They must be specified on top of the grammar, but their use is not required (so the default settings for these top-level directives take place).
 
Directives follow nearly the same syntactic rules than any other definition in UniCC. The synopsis is

``` #directive-name parameter1 parameter2 parameterN ;

Because the directive names are build-in and constant, some of them even contain blanks. The syntax of the parameter-part depends on the directive itself. The various directives are explained in detail in the following sections, besides their behavior in the different parsing modes.

+++ #!mode +++[ref_mode]
The ``#!mode`` top-level directive defines the parser construction mode UniCC should follow. Only the values ``sensitive`` and ``insensitive`` are allowed as parameters. This mode selection influences the general way how UniCC constructs or rewrites (if necessary!) the grammar, and must be specified before any other directive or grammar construct.

``` !#mode sensitive;

More about the parser construction modes UniCC provides is explained in a [separate chapter #ref_construction_modes].

+++ #!language +++[ref_language]
The ``#!language`` top-level directive specifies the implementation language to which the parser should be compiled in which language its semantics are written in. It may only be defined once on top of a grammar before any other directive (except other top-level directives) or language construct follows.

``` #!language "C" ;

The value specified here defines the parser template that is selected by the UniCC program module generator. UniCC itself does not really care about its value or meaning, due it has no language-specific decisions build-in.

The selected parser template describes various parts of the parser broken down to single parse tables cells that are used to construct parsers in nearly any possible, problem-oriented programming language.

If UniCC is forced to create a parser description file, the ``#language``-directive will only be attached as attribute.

Please read more about the UniCC code generation possibilites [in the section about the UniCC code generators #ref_codegen].

+++ #case insensitive strings +++[ref_case_insensitive_strings]
The ``#case insensitive strings`` directive switches string-based terminals to be case-insensitive or not. The directive can be called multiple times. Each time it is called and switched, subsequent string-based terminal symbol definitions are threatened according to the current case-order configuration state.

The ``#case insensitive strings`` directive allows for one parameter, which is of type boolean and accepts ``on`` and ``off``.

```
//All subsequent strings are case-insensitive!
#case insensitive strings on;

example$ -> "hello" | test; //This string can be written in any case order!

#case insensitive strings off;

test -> "world"; //This one must be in lower case order!
```

Default value is ``off``.

+++ #default action, #default epsilon action, #default value type +++
The ``#default action`` and ``#default epsilon action`` directives define a default semantic action code that is used for productions with no attached semantic block, including generated and virtual productions. They expect a code block as parameter.

```
#default action         [* @@ = @1; *] ;
#default epsilon action [* @@ = 0; *] ;

example$    -> @int ; //here, the default action will be used!
            | @float  [* printf( "%f\n" ); *] //Not here!
            | //here, default epsilon action is inserted!
            ;
```

``#default action`` and ``#default epsilon action`` can only be defined once, subsequent calls will produce a warning and are ignored.

The ``#default value type`` directives allows to define a default value type that is used for every symbol if no individual value type is specified. The parser templates for the standard UniCC code generator, and maybe subsequent code generators working on the XML output of UniCC may provide their own default value types. This directive allows to overide this default by explicitly specifying a value type triggered as default.

``` #default value type	<float> ;

+++ #copyright, #description, #parser, #prefix, #version +++[ref_prefix]
These are the simplest parser directives, which only have the purpose to store an informal string value.

``#parser``, ``#description``, ``#copyright`` and ``#version`` should be used to name and describe the parser and its version implemented by the grammar. ``#prefix`` defines a prefix value that can be inserted in function- or variable-identifiers within the generated parser to allow for various parsers in one input source file, or to meet specific symbol naming conventions in generated parser modules. When one of these directives is specified multiple times, their values will be glued together to one huge string.

```
#parser       "myBASIC";
#version      "0.43c";
#description  "A simple BASIC-compiler";
#copyright    "(C) 2011 by BasicMan";

#prefix       "mybasic";
```

All these values are only hold by UniCC and can be inserted into the parser-template or will be written to the XML output file - depending on the desired output code generator.

In the template engine, their values will expand with the template variables @@copyright, @@description, @@parser, @@prefix and @@version.

+++ #prologue, #epilogue +++[ref_prologue_epilogue]
The ``#prologue`` and ``#epilogue`` directives are used to define any program code that is inserted before and behind the parser implementation in the yielding parser module.

```
//Some includes and variables into the prologue
#prologue
[*
#include <stdlib.h>
#include <stdio.h>
#include <string.h>


static int variables[ 1000 ];
static int next_var = 0;

*]
;

//Defining the parser call in the epilogue
#epilogue
[*
int main()
{
	@@prefix_pcb	pcb;
	memset( &pcb, 0, sizeof( pcb ) );

	return @@prefix_parse( &pcb );
}
*]
;
```

In many UniCC parsers that had been extended to real compilers, these two directives take much (nearly the most!) content of the parser definition files, because they contain the surrounding code that is required to run the parser in its environment it was implemented for. It strongly depends on the implementation language and parser template how these directives are handled exactly. Multiple calls of ``#prologue`` or ``#epilogue`` are possible and glue all specified code blocks together.

Please read more annotations about the UniCC standard parser template for the C programming language and its features and interfaces, which are mostly switched and modified by C preprocessor directives that are put into ``#prologue`` and ``#epilogue`` directives.

+++ #left, #right, #nonassoc, #precedence +++[ref_precedences]
The ``#left``, ``#right`` and ``#nonassoc`` directives set associativity and precedence weightings to terminal symbols, and are used to resolve conflicts in ambiguous grammars. They influence the behavior of UniCC when constructing the parse-tables, whether to shift (right-associativity) or to reduce (left-associativity) on shift-reduce conflicts. ``#nonassoc`` defines terminal symbols not to be associative in any way - if this is tried at parser's runtime, it will throw a syntax error.

Depending on the left- or right-associativity configuration, the syntax tree grows left- or right-leaning. This can be visualized with two simple grammars and their related syntax trees on the input expression //1+2+3+4;//.

%!include: ``reference/left.par``
GENAST#reference/left.par#1+2+3+4;#-semantics#Syntax tree of input "1+2+3+4;" in a left-leaning configuration.

Now the opposite using the ``#right`` directive.

%!include: ``reference/right.par``
GENAST#reference/right.par#1+2+3+4;#-semantics#Syntax tree of input "1+2+3+4;" in a right-leaning configuration.

Multiple calls of these directives cause higher precedence leveling, so it is necessary to perform multiple calls of ``#left`` to build-up a pecedence/associativity matrix for, e.g. expressions. All symbols passed to the first ``#left`` directive call get a precedence level of 1, all symbols passed to the second ``#left`` call get a precedence level of 2, and so on.

%!include: ``reference/assoc.par``

GENAST#reference/assoc.par#1*2+3;#-semantics#Syntax tree with association precedences.

As a comparison to the above, correctly parsed syntax tree, this is the version without any associativity and precedence weightings.

GENAST#reference/no_assoc.par#1*2+3;#-semantics#Syntax tree without association precedences. UniCC compile-time warnings had been ignored.

The ``#precedence``-directive is a special-case directive. It is the only kind of directive that can be used on production level. ``#precedence`` is used to assign a terminal's precedence to a production, to let the production take a higher level of precedence in some conflicting situations. This feature was adoptet from well-known parser generators like bison.

Given a grammar, that allows for both an ary and unary minus. Correct precedence associaton has already been done for ``+``, ``-``, ``*`` and ``/``.

%!include: ``reference/no_prec.par``

Let this parser run with the expression //-2*3//, it will produce this incorrect parse tree.

GENAST#reference/no_prec.par#-2*3##The incorrect syntax tree of "-2*3".

By using the ``#precedence``-directive, the production defining the unary minus can be configured to take a higher precedence than the ary minus normally takes. Defining it

``` '-' expression            #precedence '*'

as production definition, the correct syntax tree will be constructed for //-2*3//.

GENAST#reference/prec.par#-2*3##Correct syntax tree of "-2*3", using a production predecence declaration.

The ``#precedence``-directive has to be put right beind the production and its reduction code, so 

``` '-' expression        [* @@ = -@2; *]    #precedence '*'

could be a valid reduction code submit.

+++ #whitespaces +++[ref_whitespaces]
The ``#whitespaces`` directives defines symbols to be handled as whitespace, which means they are allowed everywhere between terminals and simply ignored. In programming languages, comments, blanks, tabs and in some languages even line breaks are handled as whitespace.

``#whitespaces`` is one of the most influencing UniCC directives, and works different in both of the two parser construction modes.

++++ #whitespaces in sensitive mode ++++
If ``#whitespaces`` is used in [sensitive mode #ref_construction_modes] (default), it allows for symbols of any kind, including nonterminal symbols. This means, that whitespace can be constructed from sub-grammars which are made-up of entire grammatical rules and their subsequent rules. Using this parser construction mode, UniCC rewrites the entire grammar to make whitespace become valid in situations after a regular terminal is shifted, or at the beginning of the grammar. This grammar revisions comes visible when calling UniCC with the ``-G`` option, to print out the revised grammar that is used to finally construct the parse tables.

For example, the simple grammar

%!include: ``reference/w_sensitive.par``

yields in a revised grammar UniCC outputs as

%!include: ``reference/w_sensitive.out``

which introduces new virtual nonterminals ``&whitespace``, ``&whitespace+``, ``&whitespace*``, ``Hello'`` and ``start'``.

But it uses a fully-fledged nonterminal ``whitespace`` which could possibly exist of very flexible, grammatical constructions. The behavior of terminal symbols within the revision can also be influenced by the directives [``#lexeme`` #ref_lexeme], [``#fixate`` #ref_fixate] and [``#lexeme separation`` #ref_lexeme_separation].

Whitespace becomes part of the grammar, but its existence is hidden from the semantic actions of the grammar developer. There is no obvious indicator if whitespace handling has been done, this can only be indicated on demand.

++++ #whitespaces in insensitive mode ++++
In the [insensitive parser construction mode #ref_construction_modes], the ``#whitespaces`` directive only accepts terminal symbols. Trying to specifiy a nonterminal at ``#whitespaces`` results in an error, because whitespace consumed by the parser can't be handled here. The grammar is not rewritten as in sensitive mode.

All specified terminal symbols are flagged as whitespace, and are simply ignored when read by the resuling lexical analyzer. The grammar is not rewritten, but the grammar developer is limited to only use character-classes, strings and regular expressions as whitespaces symbols, without a flexible grammar beyond.

Resulting grammars result in much lesser states and will be parsed faster.

+++ #lexeme +++[ref_lexeme]
The ``#lexeme`` directive can only be used within the [sensitive parser construction mode #ref_construction_modes]. It configures nonterminal symbols including all their subsequent productions and terminal-/nonterminal calls to be handled as coherent lexical units, the so called //lexeme//.
``#lexeme`` directly influences the grammar revision process. Each as ``#lexeme`` configured symbol is handled like a terminal-symbol within the whitespace grammar revision, so whitespace is allowed behind a lexem, but not within.

Given is this example grammar:

%!include: ``reference/no_lexeme.par``

When this grammar is passed to UniCC, it will produce a parser that allows to parse statements like

```
print hello
print 4711
print h e ll     o  wor ld
print 1  675467     9  123
```

The two last lines are internally parsed as "helloworld" and "16754679123", but this is not the desired syntax for our language.
Adding a lexem configuration

``` #lexeme ident integer ;

to this grammar yields in a different, correct parser, which only accepts the lexeme we want to allow for.

The difference that is done during the automatic revision of the grammar caused by the ``#whitespaces``-directive in combination with the sensitive mode can be made visible using the ``-G`` flag to let UniCC print out the final grammar. The inital version of the revised grammar is this one, if no lexeme configuration is done.

%!include: ``reference/no_lexeme.out``

The rewritten nonterminals derived from the terminal symbols are extended by their origin symbol name with an appended quote ``'``.
In this case, the character-class terminals ``'A-Za-z_'`` had been rewritten to a new nonterminal with the name ``A-Za-z_'``, which allows for whitespace behind every character that matches the character-class.

The version with lexemes configured will be a little shorter, because UniCC revises not all terminals of the grammar to allow for whitespace.

%!include: ``reference/lexeme.out``

Now, nonterminals ``integer`` and ``ident`` stay on their own, they are not revised. Instead, there is now a nonterminal ``integer'`` and ``ident'`` that allows for whitespace __behind__ the lexeme. With a sharp look to the nonterminals generated to parse the whitespace, it gets more clear that whitespace-related nonterminals are themself some kind of lexeme.

It is obvious, that the ``#lexeme`` directive is one of UniCC's most powerful features in the sensitive parser construction mode, because it enables for richer grammatical constructions than any regular language allows for, with the difference that this constructs are handled as they would lexeme taken from the lexical analyzer.

If a grammar constructs with lexemes produce shift-reduce warnings, ``#lexeme`` can be used in combination with the [``#lexeme separation`` directive #ref_lexeme_separation].

+++ #lexeme separation +++[ref_lexeme_separation]
The ``#lexeme separation`` directive can only be used within the [sensitive parser construction mode #ref_construction_modes] and in combination with the [``#lexeme`` directive #ref_lexeme]. It is closely related to the [``#fixate`` directive #fixate], and configures all symbols configured as lexemes to be handled like fixate. It is a boolean directive, and can be switched ``on`` or ``off``. Calling it without boolean parameter switches ``#lexeme separation`` as ``on``.

Based on the sample grammar from above, it should be allowed for multiple values where only one value should be allowed yet.
This could be done by introducing a new virtual production.

``` start$ -> "print" value+ ;

Feeding this to UniCC and with warning reporting switched on, this causes a shift-reduce conflict in two states, because UniCC detects an ambigous grammar in both lexeme.

```
unicc: warning: state 9: Shift-reduce conflict on lookahead: 'A-Z_a-z'
    (5) ident -> 'A-Z_a-z' .ident 
    (6) ident -> 'A-Z_a-z' .      { ' ' '\n' 'A-Z_a-z' '0-9' }

unicc: warning: state 10: Shift-reduce conflict on lookahead: '0-9'
    (7) integer -> '0-9' .integer 
    (8) integer -> '0-9' .      { ' ' '\n' 'A-Z_a-z' '0-9' }
```

By using ``#lexeme separation on ;``, the warnings will be supressed, and UniCC always selects the way that input for an lexeme is consumed as much as possible.

+++ #fixate +++[ref_fixate]
The ``#fixate`` directive can only be used within the [sensitive parser construction mode #ref_construction_modes]. It supresses shift-reduce warnings that may raise up for some nonterminals caused by the grammar construction, and always prefers to shift. ``#fixate`` expects a list of nonterminal symbols to be configured as fixed.

+++ #reserve terminals +++[ref_reserve_terminals]
The ``#reserve terminals`` directive switches if [terminal anomaly detection #ref_terminal_anomalies] is performed by UniCC or not. If ``#reserve terminals`` is switched on, UniCC assumes that terminals based on regular expressions take higher precedence in all places, and does not perform terminal anomaly detection. It is a boolean directive, and can be switched ``on`` or ``off``. Calling it without boolean parameter switches ``#reserve terminals`` as ``on``.

More about terminal anomalies can be found in the section [terminal anomalies #ref_terminal_anomalies]. The directive can only be used in the [sensitive parser construction mode #ref_construction_modes].

%-------------------------------------------------------------------------------
%-------------------------------------------------------------------------------
%-------------------------------------------------------------------------------
@NEWPAGE@

++ Special symbols ++[ref_special_symbols]
UniCC grammars may contain some special terminal and nonterminal symbols under various circumstances. These special symbols begin with an ampersand ``&``, and cannot be expressed in the grammar itself, except the symbol ``&error``. Their meaning and usage is listed in the following table below.

|| Special symbol | Type | Usage |
| &embedded_<num> | nonterminal | Anonymous nonterminals get this name with a consecutive number. |
| &eof | terminal | Specifies the end-of-file symbol. This special symbol exists in every grammar, and will be automatically appended to the goal-symbol. The end-of-file symbol itself is determined by the parser, but is not bound to a special character sequence or value. In the UniCC Standard C Parser Template, the end-of-file symbol can be dynamically set to fit to the particular input. |
| &error | terminal | Specifies the error resynchonization symbol. This symbol is used to resynchronize the parser after getting scrambled by a parse error. The chapter [error recovery #ref_error_recovery] covers the use and further meaning of the error resynchonitation terminal. |
| &whitespace, &whitespace*, &whitespace+ | nonterminal | This group of nonterminals are only inserted in sensitive mode grammars that make use of the ``#whitespaces`` directive. Calls to this symbol are added to appropriate positions relating the lexeme configuration of the grammar. More about this topic can be read in chapter about the [``#whitespaces`` directive #ref_whitespaces]. |


TABLE##Special symbols in UniCC parsers.

%-------------------------------------------------------------------------------
%-------------------------------------------------------------------------------
%-------------------------------------------------------------------------------
@NEWPAGE@

++ Error recovery ++[ref_error_recovery]

Requirements to modern parsers and compilers expect, that a parse error on wrong input don't causes a final stop of the parser or compiler. As many errors as possible should be reported to the user before a correction of the input and a re-compilation process is invoked. UniCC parsers normally stop on invalid input, respective a parse error. To get a parse error resynchronized and continue parsing, the special symbol ``&error`` can be used within the grammar.

%!include: ``reference/error.par``

Error resynchronization means that the error is handled by the grammar and its semantic actions rather than the parser. In the above example, the parser will, in case of a parse error, try to pop as many items off the parse stack until it finds the error resynchronization token, ``&error``. This is shifted then, and the parser comes into a new, valid state that is covered by the grammar, so the production with the error state takes place. The parse error had been recovered, respective it is not a parse error anymore. By modifiying the actions in the semantic action of a error state, error messages can still be forced and errors counted, as it is done in above example. But the parser continues, and will output correct values on wrong input, e.g. "2*5+x".

%!include: ``reference/error.out``

``&error`` can be used on several positions, so it is also possible to report errors with it or to silently fix them and let the parser continue. It must be mentioned, that the way how parse errors and the error synchronization token is handled relies on the used target language parser template, and is not a direct feature of the UniCC Parser Generator itself.

%++ Tagging ++
%The tagging feature, also called optional augmentation, is a feature in UniCC that is not clearly specified yet, but maybe becomes important in future versions of UniCC. Tagging is the possibility to tag grammatical elements like symbols, productions and the grammar itself with additional information that could be used by subsequent analysis or code generation tasks. This feature is only interesting for the XML-based Parser Description Generator for now, because it adds these tags to appropriate positions in the XML-output, to be validated by subs
